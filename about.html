## About

<p align="left">I am a senior scientist in the 
<a href="https://aws.amazon.com/machine-learning/"> AWS AI</a> team of Amazon. 
Before joining Amazon, I worked at Parallel Computing Lab, 
<a href="https://www.intel.com/content/www/us/en/research/overview.html"> Intel Labs</a>.
I received my Ph.D. in <a href="http://www.cs.princeton.edu">computer science</a> and 
<a href="http://pni.princeton.edu">neuroscience</a> from 
<a href="http://www.princeton.edu">Princeton University</a>.</p>
    
<p align="left">My research interest is in systems, high-performance computing, and big data analytics.
I currently work on deep learning systems, with a focus on compiling and optimizing
deep learning models for efficient training and inference.
Our mission is to bridge the high-level models from various frameworks and low-level 
hardware platforms including CPUs, GPUs, and 
<a href="https://aws.amazon.com/machine-learning/inferentia/">AWS homegrown accelerators</a>, 
so that different models can execute in high-performance on different devices.
My team on one hand delivers solid solutions and systems to serve both external and internal
usages, on the other hand explores the cutting-edge techniques to conduct research and 
publish in top-tier systems conferences.</p>
    
<p align="left">Out of work, I spend some time to write a systematic 
<a href="http://tvm.d2l.ai/">deep learning compiler tutorial</a>
based on <a href="https://tvm.apache.org/">Apache TVM</a>. 
The progress is slow as we are short of hands and 
short of time. We are keen in looking for contributors to this work.</p>
    
<p align="left"><mark>We are actively hiring!</mark>
Do drop me a line if you are interested in our work.</p>