## About

<p align="left">I am an applied scientist in the 
<a href="https://aws.amazon.com/machine-learning/"> AWS AI</a> team of Amazon. 
Before joining Amazon, I worked at Parallel Computing Lab, 
<a href="https://www.intel.com/content/www/us/en/research/overview.html"> Intel Labs</a> 
as a research scientist.
I received my Ph.D. in <a href="http://www.cs.princeton.edu">computer science</a> and 
<a href="http://pni.princeton.edu">neuroscience</a> from 
<a href="http://www.princeton.edu">Princeton University</a>.
My research interest is in high-performance computing and big data analytics.
I currently work on optimizing the inference of deep learning models towards 
different kinds of hardware architecture, e.g. CPUs, GPUs, TPUs. 
I am a contributor of <a href="https://github.com/brainiak/brainiak">BrainIAK</a> and 
<a href="https://github.com/dmlc/tvm">TVM</a>.</p>

<p align="left">Together with my colleagues at AWS, we are the main maintainers of 
    <a href="https://mxnet.incubator.apache.org/">MXNet</a>, a flexible and efficient 
    library for deep learning. For Chinese users, we also maintain a 
    <a href="https://zh.mxnet.io/">Chinese MXNet website</a>, including
    <a href="http://zh.gluon.ai/">video tutorials</a>, 
    <a href="https://zh.mxnet.io/blog/">weekly updated blogs</a>, and an active 
    <a href="https://discuss.gluon.ai/">forum</a>.</p>