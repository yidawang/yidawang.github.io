## About

<p align="left">I am a principal scientist in the 
<a href="https://aws.amazon.com/machine-learning/"> AWS AI</a> team of Amazon. 
Before joining Amazon, I worked at Parallel Computing Lab, 
<a href="https://www.intel.com/content/www/us/en/research/overview.html"> Intel Labs</a>.
I received my Ph.D. in <a href="http://www.cs.princeton.edu">computer science</a> and 
<a href="http://pni.princeton.edu">neuroscience</a> from 
<a href="http://www.princeton.edu">Princeton University</a>.</p>
    
<p align="left">My research interest is in systems, high-performance computing, and big data analytics.
I currently work on deep learning systems, with a focus on compiling and optimizing
deep learning models for efficient training and inference.
Our mission is to bridge the high-level models from various frameworks and low-level 
hardware platforms including CPUs, GPUs, and 
<a href="https://aws.amazon.com/machine-learning/trainium/">AWS homegrown accelerators</a>, 
so that different models can execute in high-performance on different devices.
My team on one hand delivers solid solutions and systems to serve both external and internal
usages, on the other hand explores the cutting-edge techniques to conduct research and 
publish in top-tier systems conferences. Here is a collection of my published 
<a href="https://www.amazon.science/author/yida-wang">papers</a> in AWS.</p>
    
<p align="left"></p>
    
<p align="left"><mark>We are actively hiring!</mark>
    We are looking for talents in machine learning systems in general, ranging from distributed training,
    deep learning compiler, to tensor program generation, etc.
Do drop me a line if you are interested in our work.</p>
